[
  {
    "objectID": "data-info/sas_disease.html",
    "href": "data-info/sas_disease.html",
    "title": "SAS Disease",
    "section": "",
    "text": "To demonstrate the various types of sums of squares, we’ll create a data frame called `df_disease` taken from the SAS documentation (__reference__). The summary of the data is shown.\nThe general summary of the data is as follows\n\n\n drug   disease       y        \n 1:18   1:24    Min.   :-6.00  \n 2:18   2:24    1st Qu.: 9.00  \n 3:18   3:24    Median :21.00  \n 4:18           Mean   :18.88  \n                3rd Qu.:28.00  \n                Max.   :44.00  \n                NA's   :14     \n\n\n\nData summary\n\n\nName\ndf_disease\n\n\nNumber of rows\n72\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndrug\n0\n1\nFALSE\n4\n1: 18, 2: 18, 3: 18, 4: 18\n\n\ndisease\n0\n1\nFALSE\n3\n1: 24, 2: 24, 3: 24\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ny\n14\n0.81\n18.88\n12.8\n-6\n9\n21\n28\n44\n▅▆▆▇▂"
  },
  {
    "objectID": "templates/R_template.html",
    "href": "templates/R_template.html",
    "title": "R Template",
    "section": "",
    "text": "Optional If there is only one available package this can be deleted. Otherwise please make a short list, paragraph or table. If there is a reason to use one package vs another please include it. Please make sure to include what version of the packages you are using\n\n\n\nJust a sentence or two about the data and a link to the data page\n\n\n\n\n\nOptional if there is more than one package"
  },
  {
    "objectID": "templates/R_template.html#available-r-packages",
    "href": "templates/R_template.html#available-r-packages",
    "title": "R Template",
    "section": "",
    "text": "Optional If there is only one available package this can be deleted. Otherwise please make a short list, paragraph or table. If there is a reason to use one package vs another please include it. Please make sure to include what version of the packages you are using"
  },
  {
    "objectID": "templates/R_template.html#data-used",
    "href": "templates/R_template.html#data-used",
    "title": "R Template",
    "section": "",
    "text": "Just a sentence or two about the data and a link to the data page"
  },
  {
    "objectID": "templates/R_template.html#example-code",
    "href": "templates/R_template.html#example-code",
    "title": "R Template",
    "section": "",
    "text": "Optional if there is more than one package"
  },
  {
    "objectID": "SAS/mmrm.html",
    "href": "SAS/mmrm.html",
    "title": "MMRM in SAS",
    "section": "",
    "text": "Mixed Models\n\nFitting the MMRM in SAS\nIn SAS the following code was used (assessments at avisitn=0 should also be removed from the response variable):\n\nproc mixed data=adlbh;\n  where base ne . and avisitn not in (., 99);\n  class usubjid trtpn(ref=\"0\") avisitn;\n  by paramcd param;\n  model chg=base trtpn avisitn  trtpn*avisitn / solution cl alpha=0.05 ddfm=KR;\n  repeated avisitn/subject=usubjid type=&covar;\n  lsmeans trtpn * avisitn / diff cl slice=avisitn;\n  lsmeans trtpn / diff cl;\nrun;\n\nwhere the macro variable covar could be UN, CS or AR(1). The results were stored in .csv files that were post-processed in R and compared with the results from R."
  },
  {
    "objectID": "SAS/linear-models.html",
    "href": "SAS/linear-models.html",
    "title": "linear-models",
    "section": "",
    "text": "Getting Started\nTo demonstrate the various types of sums of squares, we'll create a data frame called df_disease taken from the SAS documentation (reference). For more information/to investigate this data go here\n\n\nThe Model\nFor this example, we’re testing for a significant difference in stem_length using ANOVA.\n\nproc glm;\n   class drug disease;\n   model y=drug disease drug*disease;\nrun;\n\n\n\nSums of Squares Tables\nSAS has four types of sums of squares calculations. To get these calculations, the sum of squares option needs to be added (/ ss1 ss2 ss3 ss4) to the model statement.\n\nproc glm;\n   class drug disease;\n   model y=drug disease drug*disease / ss1 ss2 ss3 ss4;\nrun;\n\n\nType I\n\n\n\n\n\n\n\n\n\n\n\nType II\n\n\n\n\n\n\n\n\n\n\n\nType III\n\n\n\n\n\n\n\n\n\n\n\nType IV"
  },
  {
    "objectID": "SAS/summary-stats.html",
    "href": "SAS/summary-stats.html",
    "title": "Deriving Quantiles or Percentiles in SAS",
    "section": "",
    "text": "Percentiles can be calculated in SAS using the UNIVARIATE procedure. The procedure has the option PCTLDEF which allows for five different percentile definitions to be used. The default is PCTLDEF=5, which uses the empirical distribution function to find percentiles.\nThis is how the 25th and 40th percentiles of aval in the dataset adlb could be calculated, using the default option for PCTLDEF.\n\nproc univariate data=adlb;\n  var aval;\n  output out=stats pctlpts=25 40 pctlpre=p;\nrun;\n\nThe pctlpre=p option tells SAS the prefix to use in the output dataset for the percentile results. In the above example, SAS will create a dataset called stats, containing variables p25 and p40."
  },
  {
    "objectID": "SAS/rounding.html",
    "href": "SAS/rounding.html",
    "title": "Rounding in SAS",
    "section": "",
    "text": "There are two rounding functions in SAS.\nThe round() function in SAS will round to the nearest whole number and ‘away from zero’ or ‘rounding up’ when equidistant meaning that exactly 12.5 rounds to the integer 13.\nThe rounde() function in SAS will round to the nearest whole number and ‘rounding to the even number’ when equidistant, meaning that exactly 12.5 rounds to the integer 12.\nBoth functions allow you to specify the number of decimal places you want to round to.\nFor example (See references for source of the example)\n\n    #Example code\n    data XXX;\n      my_number=2.2; output;\n      my_number=3.99; output;\n      my_number=1.2345; output;\n      my_number=7.876; output;\n      my_number=13.8739;  output;\n    run;\n\n    data xxx2;\n      set xxx;\n        r_1_dec = round(my_number, 0.1);\n        r_2_dec = round(my_number, 0.01);\n        r_3_dec = round(my_number, 0.001);\n        \n        re_1_dec = rounde(my_number, 0.1);\n        re_2_dec = rounde(my_number, 0.01);\n        re_3_dec = rounde(my_number, 0.001);\n    run;\n\n\n\n\n\n\n\n\n\n\n\n\n\nmy_number\nr_1_dec\nr_2_de\nr_3_dec\nre_1_dec\nre_2_dec\nre_3_dec\n\n\n\n\n2.2\n2.2\n2.2\n2.2\n2.2\n2.2\n2.2\n\n\n3.99\n4\n3.99\n3.99\n4\n3.99\n3.99\n\n\n1.2345\n1.2\n1.23\n1.235\n1.2\n1.23\n1.234\n\n\n7.876\n7.9\n7.88\n7.876\n7.9\n7.88\n7.876\n\n\n13.8739\n13.9\n13.87\n13.874\n13.9\n13.87\n13.874\n\n\n\nReferences\nHow to Round Numbers in SAS - SAS Example Code"
  },
  {
    "objectID": "SAS/mcnemar.html",
    "href": "SAS/mcnemar.html",
    "title": "McNemar’s test in SAS",
    "section": "",
    "text": "Performing McNemar’s test in SAS\nTo demonstrate McNemar’s test in SAS, data concerning the presence or absence of cold symptoms was used. The symptoms were recorded by the same children at the age of 12 and 14. A total of 2638 participants were involved.\n\nUsing PROC FREQ\nTesting for a significant difference in cold symptoms between ages, using McNemar’s test in SAS, can be performed as below. The AGREE option is stated within the FREQ procedure to produce agreement tests and measures, including McNemar’s test.\n\nproc freq data=colds;\n  tables age12*age14 / agree;\nrun;\n\n\n\nResults\n\n\n\n\n\n\n\n\n\nSAS outputs the tabulated data for proportions, the McNemar’s Chi-square statistic, and the Kappa coefficient with 95% confidence limits. There is no continuity correction used and no option to include this."
  },
  {
    "objectID": "SAS/manova.html",
    "href": "SAS/manova.html",
    "title": "Multivariate Analysis of Variance in SAS",
    "section": "",
    "text": "Example 39.6 Multivariate Analysis of Variance from SAS MANOVA User Guide\nThis example employs multivariate analysis of variance (MANOVA) to measure differences in the chemical characteristics of ancient pottery found at four kiln sites in Great Britain. The data are from Tubb, Parker, and Nickless (1980), as reported in Hand et al. (1994).\nFor each of 26 samples of pottery, the percentages of oxides of five metals are measured. The following statements create the data set and invoke the GLM procedure to perform a one-way MANOVA. Additionally, it is of interest to know whether the pottery from one site in Wales (Llanederyn) differs from the samples from other sites; a CONTRAST statement is used to test this hypothesis.\n\n    #Example code\n    title \"Romano-British Pottery\";\n   data pottery;\n      input Site $12. Al Fe Mg Ca Na;\n      datalines;\n   Llanederyn   14.4 7.00 4.30 0.15 0.51\n   Llanederyn   13.8 7.08 3.43 0.12 0.17\n   Llanederyn   14.6 7.09 3.88 0.13 0.20\n   Llanederyn   11.5 6.37 5.64 0.16 0.14\n   Llanederyn   13.8 7.06 5.34 0.20 0.20\n   Llanederyn   10.9 6.26 3.47 0.17 0.22\n   Llanederyn   10.1 4.26 4.26 0.20 0.18\n   Llanederyn   11.6 5.78 5.91 0.18 0.16\n   Llanederyn   11.1 5.49 4.52 0.29 0.30\n   Llanederyn   13.4 6.92 7.23 0.28 0.20\n   Llanederyn   12.4 6.13 5.69 0.22 0.54\n   Llanederyn   13.1 6.64 5.51 0.31 0.24\n   Llanederyn   12.7 6.69 4.45 0.20 0.22\n   Llanederyn   12.5 6.44 3.94 0.22 0.23\n   Caldicot     11.8 5.44 3.94 0.30 0.04\n   Caldicot     11.6 5.39 3.77 0.29 0.06\n   IslandThorns 18.3 1.28 0.67 0.03 0.03\n   IslandThorns 15.8 2.39 0.63 0.01 0.04\n   IslandThorns 18.0 1.50 0.67 0.01 0.06\n   IslandThorns 18.0 1.88 0.68 0.01 0.04\n   IslandThorns 20.8 1.51 0.72 0.07 0.10\n   AshleyRails  17.7 1.12 0.56 0.06 0.06\n   AshleyRails  18.3 1.14 0.67 0.06 0.05\n   AshleyRails  16.7 0.92 0.53 0.01 0.05\n   AshleyRails  14.8 2.74 0.67 0.03 0.05\n   AshleyRails  19.1 1.64 0.60 0.10 0.03\n   ;\n   run;\n   \n   proc glm data=pottery;\n      class Site;\n      model Al Fe Mg Ca Na = Site;\n      contrast 'Llanederyn vs. the rest' Site 1 1 1 -3;\n      manova h=_all_ / printe printh;\n   run;\n\nAfter the summary information (1), PROC GLM produces the univariate analyses for each of the dependent variables (2-6). These analyses show that sites are significantly different for all oxides individually. You can suppress these univariate analyses by specifying the NOUNI option in the MODEL statement.\n1 Summary Information about Groups\n\n\n\n\n\n\n\n\n\n2 Univariate Analysis of Variance for Aluminum Oxide (AI)\n\n\n\n\n\n\n\n\n\n3 Univariate Analysis of Variance for Iron Oxide (Fe)\n\n\n\n\n\n\n\n\n\n4 Univariate Analysis of Variance for Calcium Oxide (Ca)\n\n\n\n\n\n\n\n\n\n5 Univariate Analysis of Variance for Magnesium Oxide (Mg)\n\n\n\n\n\n\n\n\n\n6 Analysis of Variance for Sodium Oxide (Na)\n\n\n\n\n\n\n\n\n\nThe PRINTE option in the MANOVA statement displays the elements of the error matrix (7), also called the Error Sums of Squares and Crossproducts matrix. The diagonal elements of this matrix are the error sums of squares from the corresponding univariate analyses.\nThe PRINTE option also displays the partial correlation matrix (7) associated with the E matrix. In this example, none of the oxides are very strongly correlated; the strongest correlation (r=0.488) is between magnesium oxide and calcium oxide.\n7 Error SSCP Matrix and Partial Correlations\n\n\n\n\n\n\n\n\n\nThe PRINTH option produces the SSCP matrix for the hypotheses being tested (Site and the contrast); (8 and 9). Since the Type III SS are the highest-level SS produced by PROC GLM by default, and since the HTYPE= option is not specified, the SSCP matrix for Site gives the Type III H matrix. The diagonal elements of this matrix are the model sums of squares from the corresponding univariate analyses.\nFour multivariate tests are computed, all based on the characteristic roots and vectors of \\(E^{-1}H\\). These roots and vectors are displayed along with the tests. All four tests can be transformed to variates that have distributions under the null hypothesis. Note that the four tests all give the same results for the contrast, since it has only one degree of freedom. In this case, the multivariate analysis matches the univariate results: there is an overall difference between the chemical composition of samples from different sites, and the samples from Llanederyn are different from the average of the other sites.\n8 Hypothesis SSCP Matrix and Multivariate Tests for Overall Site Effect\n\n\n\n\n\n\n\n\n\n9 Hypothesis SSCP Matrix and Multivariate Tests for Differences between Llanederyn and the Other Sites\n\n\n\n\n\n\n\n\n\nReferences\nSAS MANOVA User Guide"
  },
  {
    "objectID": "R/mmrm.html",
    "href": "R/mmrm.html",
    "title": "MMRM in R",
    "section": "",
    "text": "Fitting the MMRM in R\n\nUsing the nlme::gls function\nThe code below implements an MMRM fit in R with the nlme::gls function.\n\ngls(model = CHG ~ TRTP + AVISITN + TRTP:AVISITN + AVISITN + BASE,\n    data = data,\n    correlation = corSymm(form = ~1|SUBJID),\n    weights = varIdent(form = ~1|AVISITN),\n    control = glsControl(opt = \"optim\"),\n    method = \"REML\",\n    na.action = \"na.omit\")\n\nhere we can swap out corSymm for corCompSymm to give the compound symmetry structure or corCAR1 for autoregressive of first order (AR(1)).\n\n\nUsing the lme4::lmer function\nAn alternative way to fit an MMRM with unstructured covariance matrices is to use the lme4::lmer function as described by Daniel Sabanes Bove in his R in Pharma talk from 2020 see here. The relevance of this fit is apparent when we consider the availability of the Kenward-Roger’s degrees of freedom for the MMRM in R, which at the time of writing, were not yet available for the nlme::gls function via the pbkrtest package (see here).\n\nlmer(CHG ~ TRTA * VISIT + VISIT + BASE + (0 + VISIT|SUBJID),\n     data = data,\n     control = lmerControl(check.nobs.vs.nRE = \"ignore\"),\n     na.action = na.omit)\n\n\n\nExtracting effect estimates using emmeans\nIn order to extract relevant marginal means (LSmeans) and contrasts we can use the emmeans package. Below we start by constructing a ref_grid used to make explicit just how the predictions are generated across the levels of TRTP and AVISITN. The emmeans function permits various marginal means to be extracted depending on the formula provided and the following pairs() function call derives relevant contrasts. Note that more control can be obtained by calling the contrast() function.\n\nmod_grid &lt;- ref_grid(model, data = data, mode = \"df.error\")\nmod_emm &lt;- emmeans(mod_grid, ~TRTP * AVISITN, mode = \"df.error\") \npairs(mod_emm)"
  },
  {
    "objectID": "R/linear-models.html",
    "href": "R/linear-models.html",
    "title": "linear-models",
    "section": "",
    "text": "Getting Started\nTo demonstrate the various types of sums of squares, we'll create a data frame called df_disease taken from the SAS documentation (reference). For more information/to investigate this data go here\n\n\nThe Model\nFor this example, we’re testing for a significant difference in stem_length using ANOVA. In R, we’re using lm() to run the ANOVA, and then using broom::glance() and broom::tidy() to view the results in a table format.\n\nlm_model &lt;- lm(y ~ drug + disease + drug*disease, df_disease)\n\nThe glance function gives us a summary of the model diagnostic values.\n\nlm_model %&gt;% \n  glance() %&gt;% \n  pivot_longer(everything())\n\n# A tibble: 12 × 2\n   name               value\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 r.squared        0.456  \n 2 adj.r.squared    0.326  \n 3 sigma           10.5    \n 4 statistic        3.51   \n 5 p.value          0.00130\n 6 df              11      \n 7 logLik        -212.     \n 8 AIC            450.     \n 9 BIC            477.     \n10 deviance      5081.     \n11 df.residual     46      \n12 nobs            58      \n\n\nThe tidy function gives a summary of the model results.\n\nlm_model %&gt;% tidy()\n\n# A tibble: 12 × 5\n   term           estimate std.error statistic      p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)      29.3        4.29    6.84   0.0000000160\n 2 drug2            -1.33       6.36   -0.210  0.835       \n 3 drug3           -13          7.43   -1.75   0.0869      \n 4 drug4           -15.7        6.36   -2.47   0.0172      \n 5 disease2         -1.08       6.78   -0.160  0.874       \n 6 disease3         -8.93       6.36   -1.40   0.167       \n 7 drug2:disease2    6.58       9.78    0.673  0.504       \n 8 drug3:disease2  -10.9       10.2    -1.06   0.295       \n 9 drug4:disease2    0.317      9.30    0.0340 0.973       \n10 drug2:disease3   -0.900      9.00   -0.100  0.921       \n11 drug3:disease3    1.10      10.2     0.107  0.915       \n12 drug4:disease3    9.53       9.20    1.04   0.306       \n\n\n\n\nThe Results\nYou’ll see that R print the individual results for each level of the drug and disease interaction. We can get the combined F table in R using the anova() function on the model object.\n\nlm_model %&gt;% \n  anova() %&gt;% \n  tidy() %&gt;% \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ndrug\n3\n3133.2385\n1044.4128\n9.455761\n0.0000558\n\n\ndisease\n2\n418.8337\n209.4169\n1.895990\n0.1617201\n\n\ndrug:disease\n6\n707.2663\n117.8777\n1.067225\n0.3958458\n\n\nResiduals\n46\n5080.8167\n110.4525\nNA\nNA\n\n\n\n\n\nWe can add a Total row, by using add_row and calculating the sum of the degrees of freedom and sum of squares.\n\nlm_model %&gt;%\n  anova() %&gt;%\n  tidy() %&gt;%\n  add_row(term = \"Total\", df = sum(.$df), sumsq = sum(.$sumsq)) %&gt;% \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ndrug\n3\n3133.2385\n1044.4128\n9.455761\n0.0000558\n\n\ndisease\n2\n418.8337\n209.4169\n1.895990\n0.1617201\n\n\ndrug:disease\n6\n707.2663\n117.8777\n1.067225\n0.3958458\n\n\nResiduals\n46\n5080.8167\n110.4525\nNA\nNA\n\n\nTotal\n57\n9340.1552\nNA\nNA\nNA\n\n\n\n\n\n\n\nSums of Squares Tables\nUnfortunately, it is not easy to get the various types of sums of squares calculations in using functions from base R. However, the rstatix package offers a solution to produce these various sums of squares tables. For each type, you supply the original dataset and model to the. anova_test function, then speccify the ttype and se detailed = TRUE.\n\nType I\n\ndf_disease %&gt;% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 1, \n    detailed = TRUE) %&gt;% \n  rstatix::get_anova_table() %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nDFn\nDFd\nSSn\nSSd\nF\np\np&lt;.05\nges\n\n\n\n\ndrug\n3\n46\n3133.239\n5080.817\n9.456\n5.58e-05\n*\n0.381\n\n\ndisease\n2\n46\n418.834\n5080.817\n1.896\n1.62e-01\n\n0.076\n\n\ndrug:disease\n6\n46\n707.266\n5080.817\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType II\n\ndf_disease %&gt;% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 2, \n    detailed = TRUE) %&gt;% \n  rstatix::get_anova_table() %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\ndrug\n3063.433\n5080.817\n3\n46\n9.245\n6.75e-05\n*\n0.376\n\n\ndisease\n418.834\n5080.817\n2\n46\n1.896\n1.62e-01\n\n0.076\n\n\ndrug:disease\n707.266\n5080.817\n6\n46\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType III\n\ndf_disease %&gt;% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 3, \n    detailed = TRUE) %&gt;% \n  rstatix::get_anova_table() %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\n(Intercept)\n20037.613\n5080.817\n1\n46\n181.414\n0.00e+00\n*\n0.798\n\n\ndrug\n2997.472\n5080.817\n3\n46\n9.046\n8.09e-05\n*\n0.371\n\n\ndisease\n415.873\n5080.817\n2\n46\n1.883\n1.64e-01\n\n0.076\n\n\ndrug:disease\n707.266\n5080.817\n6\n46\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType IV\nIn R there is no equivalent operation to the Type IV sums of squares calculation in SAS."
  },
  {
    "objectID": "R/summary-stats.html",
    "href": "R/summary-stats.html",
    "title": "Deriving Quantiles or Percentiles in R",
    "section": "",
    "text": "Percentiles can be calculated in R using the quantile function. The function has the argument type which allows for nine different percentile definitions to be used. The default is type = 7, which uses a piecewise-linear estimate of the cumulative distribution function to find percentiles.\nThis is how the 25th and 40th percentiles of aval could be calculated using the default type.\n\nquantile(aval, probs = c(0.25, 0.4))"
  },
  {
    "objectID": "R/rounding.html",
    "href": "R/rounding.html",
    "title": "Rounding in R",
    "section": "",
    "text": "The round() function in Base R will round to the nearest whole number and ‘rounding to the even number’ when equidistant, meaning that exactly 12.5 rounds to the integer 12.\nNote that the janitor package in R contains a function round_half_up() that rounds away from zero. in this case it rounds to the nearest whole number and ‘away from zero’ or ‘rounding up’ when equidistant, meaning that exactly 12.5 rounds to the integer 13.\n\n#Example code\nmy_number &lt;-c(2.2,3.99,1.2345,7.876,13.8739)\n\nr_0_dec &lt;- round(my_number, digits=0);\nr_1_dec &lt;- round(my_number, digits=1);\nr_2_dec &lt;- round(my_number, digits=2);\nr_3_dec &lt;- round(my_number, digits=3);\n\nr_0_dec\nr_1_dec\nr_2_dec\nr_3_dec\n\n&gt; r_0_dec\n[1]  2  4  1  8 14\n&gt; r_1_dec\n[1]  2.2  4.0  1.2  7.9 13.9\n&gt; r_2_dec\n[1]  2.20  3.99  1.23  7.88 13.87\n&gt; r_3_dec\n[1]  2.200  3.990  1.234  7.876 13.874\n\nIf using the janitor package in R, and the function round_half_up(), the results would be the same with the exception of rounding 1.2345 to 3 decimal places where a result of 1.235 would be obtained instead of 1.234."
  },
  {
    "objectID": "R/mcnemar.html",
    "href": "R/mcnemar.html",
    "title": "McNemar’s test in R",
    "section": "",
    "text": "Performing McNemar’s test in R\nTo demonstrate McNemar’s test, data was used concerning the presence or absence of cold symptoms reported by the same children at age 12 and age 14. A total of 2638 participants were involved.\n\nUsing the epibasix::mcnemar function\nTesting for a significant difference in cold symptoms between the two ages using the mcNemar function from the epibasix package can be performed as below. The symptoms for participants at age 12 and age 14 are tabulated and stored as an object, then passed to the mcNemar function. A more complete view of the output is achieved by calling the summary function.\n\nlibrary(epibasix)\n\nX &lt;- table(colds$age12, colds$age14)\nepi_mcn &lt;- mcNemar(X)\nsummary(epi_mcn)\n\n\nMatched Pairs Analysis: McNemar's Statistic and Odds Ratio (Detailed Summary):\n \n     \n       No Yes\n  No  707 256\n  Yes 144 212\n\nEntries in above matrix correspond to number of pairs. \n \nMcNemar's Chi^2 Statistic (corrected for continuity) = 30.802 which has a p-value of: 0\nNote: The p.value for McNemar's Test corresponds to the hypothesis test: H0: OR = 1 vs. HA: OR != 1\nMcNemar's Odds Ratio (b/c): 1.778\n95% Confidence Limits for the OR are: [1.449, 2.208]\nThe risk difference is: 0.085\n95% Confidence Limits for the rd are: [0.055, 0.115]\n\n\n\n\nUsing the stats::mcnemar.test function\nMcNemar’s test can also be performed using stats::mcnemar.test as shown below, using the same table X as in the previous section.\n\nmcnemar.test(X)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  X\nMcNemar's chi-squared = 30.802, df = 1, p-value = 2.857e-08\n\n\nThe result is shown without continuity correction by specifying correct=FALSE.\n\nmcnemar.test(X, correct=FALSE)\n\n\n    McNemar's Chi-squared test\n\ndata:  X\nMcNemar's chi-squared = 31.36, df = 1, p-value = 2.144e-08\n\n\n\n\nResults\nAs default, using summary with epibasix::mcNemar gives additional information to the McNemar’s chi-square statistic. This includes a table to view proportions, and odds ratio and risk difference with 95% confidence limits. The result uses Edward’s continuity correction without the option to remove this, which is consistent with other functions within the package.\nstats::mcnemar.test uses a continuity correction as default but does allow for this to be removed. This function does not output any other coefficients for agreement or proportions but (if required) these can be achieved within other functions or packages in R."
  },
  {
    "objectID": "R/manova.html",
    "href": "R/manova.html",
    "title": "Multivariate Analysis of Variance in R",
    "section": "",
    "text": "For a detailed description of MANOVA including assumptions see Renesh Bedre\nExample 39.6 Multivariate Analysis of Variance from SAS MANOVA User Guide\nThis example employs multivariate analysis of variance (MANOVA) to measure differences in the chemical characteristics of ancient pottery found at four kiln sites in Great Britain. The data are from Tubb, Parker, and Nickless (1980), as reported in Hand et al. (1994).\nFor each of 26 samples of pottery, the percentages of oxides of five metals are measured. The following statements create the data set and perform a one-way MANOVA. Additionally, it is of interest to know whether the pottery from one site in Wales (Llanederyn) differs from the samples from other sites.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)\nlibrary(emmeans)\n\nknitr::opts_chunk$set(echo = TRUE, cache = TRUE)\npottery &lt;- read.csv(\"../data/manova1.csv\")\npottery\n\n           site   al   fe   mg   ca   na\n1    Llanederyn 14.4 7.00 4.30 0.15 0.51\n2    Llanederyn 13.8 7.08 3.43 0.12 0.17\n3    Llanederyn 14.6 7.09 3.88 0.13 0.20\n4    Llanederyn 11.5 6.37 5.64 0.16 0.14\n5    Llanederyn 13.8 7.06 5.34 0.20 0.20\n6    Llanederyn 10.9 6.26 3.47 0.17 0.22\n7    Llanederyn 10.1 4.26 4.26 0.20 0.18\n8    Llanederyn 11.6 5.78 5.91 0.18 0.16\n9    Llanederyn 11.1 5.49 4.52 0.29 0.30\n10   Llanederyn 13.4 6.92 7.23 0.28 0.20\n11   Llanederyn 12.4 6.13 5.69 0.22 0.54\n12   Llanederyn 13.1 6.64 5.51 0.31 0.24\n13   Llanederyn 12.7 6.69 4.45 0.20 0.22\n14   Llanederyn 12.5 6.44 3.94 0.22 0.23\n15     Caldicot 11.8 5.44 3.94 0.30 0.04\n16     Caldicot 11.6 5.39 3.77 0.29 0.06\n17 IslandThorns 18.3 1.28 0.67 0.03 0.03\n18 IslandThorns 15.8 2.39 0.63 0.01 0.04\n19 IslandThorns 18.0 1.50 0.67 0.01 0.06\n20 IslandThorns 18.0 1.88 0.68 0.01 0.04\n21 IslandThorns 20.8 1.51 0.72 0.07 0.10\n22  AshleyRails 17.7 1.12 0.56 0.06 0.06\n23  AshleyRails 18.3 1.14 0.67 0.06 0.05\n24  AshleyRails 16.7 0.92 0.53 0.01 0.05\n25  AshleyRails 14.8 2.74 0.67 0.03 0.05\n26  AshleyRails 19.1 1.64 0.60 0.10 0.03\n\n\n1 Perform one way MANOVA\nResponse ID for ANOVA is order of 1=al, 2=fe, 3=mg, ca, na.\nWe are testing H0: group mean vectors are the same for all groups or they dont differ significantly vs\nH1: At least one of the group mean vectors is different from the rest.\n\ndep_vars &lt;- cbind(pottery$al,pottery$fe,pottery$mg, pottery$ca, pottery$na)\nfit &lt;-manova(dep_vars ~ pottery$site)\nsummary.aov(fit)\n\n Response 1 :\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \npottery$site  3 175.610  58.537  26.669 1.627e-07 ***\nResiduals    22  48.288   2.195                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 2 :\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \npottery$site  3 134.222  44.741  89.883 1.679e-12 ***\nResiduals    22  10.951   0.498                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 3 :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npottery$site  3 103.35  34.450   49.12 6.452e-10 ***\nResiduals    22  15.43   0.701                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 4 :\n             Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \npottery$site  3 0.204703 0.068234  29.157 7.546e-08 ***\nResiduals    22 0.051486 0.002340                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 5 :\n             Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \npottery$site  3 0.25825 0.086082  9.5026 0.0003209 ***\nResiduals    22 0.19929 0.009059                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n‘summary(fit)’ outputs the MANOVA testing of an overall site effect.\nP&lt;0.001 suggests there is an overall difference between the chemical composition of samples from different sites.\n\nsummary(fit)\n\n             Df Pillai approx F num Df den Df    Pr(&gt;F)    \npottery$site  3 1.5539   4.2984     15     60 2.413e-05 ***\nResiduals    22                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n2 Now we test to see if the Llanaderyn site is different to the other sites\nNOTE: interest may now lie in using pre-planned contrast statements to investigate if one site differs when compared to the average of the others. You would imagine this could be done using the ‘contrast’ function something like the code below, however this result does not match the SAS user guide and so looks to be doing a different analysis. SUGGEST THIS IS NOT USED UNTIL MORE RESEARCH INTO THIS METHOD CAN BE PERFORMED. One alternative suggestion is to perform a linear descriminent analysis (LDA).\n\nmanova(dep_vars ~ pottery$site) %&gt;% \n          emmeans(\"site\") %&gt;% \n     contrast(method=list(\n          \"Llanederyn vs other sites\"= c(\"Llanederyn\"=-3, \"Caldicot\"=1, \"IslandThorns\"=1, \"AshleyRails\"=1)))\n\n contrast                  estimate    SE df t.ratio p.value\n Llanederyn vs other sites     1.51 0.661 22   2.288  0.0321\n\nResults are averaged over the levels of: rep.meas \n\n\nNOTE: if you feel you can help with the above discrepancy please contribute to the CAMIS repo by following the instructions on the contributions page."
  },
  {
    "objectID": "Comp/r-sas_mmrm.html",
    "href": "Comp/r-sas_mmrm.html",
    "title": "R vs SAS MMRM",
    "section": "",
    "text": "Data\nThe data used for this comparison was the lab ADaM dataset adlbh.xpt from the Phuse Pilot Study. Results were generated for each lab parameter and time point in the dataset using three different covariance structures, i.e. unstructured, compound symmetry and autoregressive of first order (AR(1)).\n\n\nComparison between SAS and R\nWith results available for SAS and R model fits, we turn our attention to generating some visual comparisons of the results. Note that here we adopt a Bland-Altman type plot which plots the difference on the y-axis and the average on the x-axis. This offers a way to inspect any bias or relationships with the size of effect and the associated bias.\nFor the extracted LS-means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand corresponding SEs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the derived contrasts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand corresponding 95%CI widths\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of SAS and R Comparison\nUsing SAS PROC MIXED and R functions such as gls, lmer, mod_grid, and mod_emm, results were broadly aligned. Results not being exact can be attributed to many factors such as rounding precision, data handling, and many other internal processing nuances. However, Bland-Altman type plots showed small but randomly distributed differences across a broad range of parameters from the input data. Apart from a small subset of the parameters, there were no trends observed which would have suggested systemic differences between the languages. These analyses were based on a single set of data so more research must be done. However, based on comparing R documentation with SAS documentation, as well as the results displayed above in this paper, it is evident that the R and the SAS methods cover do produce similarly valid results for the options which were tested.\n\n\nFuture work\n\nRun SAS code by also removing assessments at avisitn=0 from the response variable, and using trtp (or trtpn) and avisit (or avisitn)\nInvestigating the differences\nImplement lmer equivalent to MMRM with compound symmetry\nComparisons for other models, i.e. only random, random and repeated, no repeated"
  },
  {
    "objectID": "Comp/r-sas-summary-stats.html",
    "href": "Comp/r-sas-summary-stats.html",
    "title": "Deriving Quantiles or Percentiles in R vs SAS",
    "section": "",
    "text": "Data\nThe following data will be used show the differences between the default percentile definitions used by SAS and R:\n\n10, 20, 30, 40, 150, 160, 170, 180, 190, 200\n\n\n\nSAS Code\nAssuming the data above is stored in the variable aval within the dataset adlb, the 25th and 40th percentiles could be calculated using the following code.\n\nproc univariate data=adlb;\n  var aval;\n  output out=stats pctlpts=25 40 pctlpre=p;\nrun;\n\nThis procedure creates the dataset stats containing the variables p25 and p40.\n\n\n\n\n\n\n\n\n\nThe procedure has the option PCTLDEF which allows for five different percentile definitions to be used. The default is PCTLDEF=5.\n\n\nR code\nThe 25th and 40th percentiles of aval can be calculated using the quantile function.\n\nquantile(adlb$aval, probs = c(0.25, 0.4))\n\nThis gives the following output.\n\n\n  25%   40% \n 32.5 106.0 \n\n\nThe function has the argument type which allows for nine different percentile definitions to be used. The default is type = 7.\n\n\nComparison\nThe default percentile definition used by the UNIVARIATE procedure in SAS finds the 25th and 40th percentiles to be 30 and 95. The default definition used by R finds these percentiles to be 32.5 and 106.\nIt is possible to get the quantile function in R to use the same definition as the default used in SAS, by specifying type=2.\n\nquantile(adlb$aval, probs = c(0.25, 0.4), type=2)\n\nThis gives the following output.\n\n\n25% 40% \n 30  95 \n\n\nIt is not possible to get the UNIVARIATE procedure in SAS to use the same definition as the default used in R.\nRick Wicklin provided a blog post showing how SAS has built in support for calculations using 5 of the 9 percentile definitions available in R, and also demonstrated how you can use a SAS/IML function to calculate percentiles using the other 4 definitions.\nMore information about quantile derivation can be found in the SAS blog.\n\n\nKey references:\nCompare the default definitions for sample quantiles in SAS, R, and Python\nSample quantiles: A comparison of 9 definitions\nHyndman, R. J., & Fan, Y. (1996). Sample quantiles in statistical packages. The American Statistician, 50(4), 361-365."
  },
  {
    "objectID": "Comp/r-sas_mcnemar.html",
    "href": "Comp/r-sas_mcnemar.html",
    "title": "R v SAS McNemar’s test",
    "section": "",
    "text": "McNemar’s test; R and SAS\nIn R, the mcNemar function from the epibasix package can be used to perform McNemar’s test.\n\nX&lt;-table(colds$age12,colds$age14)\nsummary(mcNemar(X))\n\nThe FREQ procedure can be used in SAS with the AGREE option to run the McNemar test, with OR, and RISKDIFF options stated for production of odds ratios and risk difference. These options were added as epibasix::mcNemar outputs the odds ratio and risk difference with confidence limits as default. In contrast to R, SAS outputs the Kappa coefficients with confident limits as default.\n\nproc freq data=colds;\n    tables age12*age14 / agree or riskdiff;\nrun;\n\nWhen calculating the odds ratio and risk difference confidence limits, SAS is not treating the data as matched-pairs. There is advice on the SAS blog and SAS support page to amend this, which requires a lot of additional coding.\nR is using Edward’s continuity correction with no option to remove this. In contrast, there is no option to include Edward’s continuity correction in SAS, but this can be manually coded to agree with R. However, its use is controversial due to being seen as overly conservative.\nR’s use of the continuity correction is consistent with other functions within the epibasix package, which was categorised as ‘High Risk’ by the Risk Assessment Shiny App created by the R Validation Hub. Risk is quantified by the app through a number of metrics relating to maintenance and community usage. It was found that the author is no longer maintaining the package and there was no documentation available for certain methods used. Therefore, the use of the epibasix package is advised against and other packages may be more suitable.\nThe mcnemar.test function in the stats package provides the option to remove continuity corrections which results in a match with SAS. This function does not output any other coefficients for agreement/difference in proportions etc. but (if required) these can be achieved within other functions and/or packages.\n\nmcnemar.test(X, correct = FALSE)"
  },
  {
    "objectID": "Comp/r-sas_linear-models.html",
    "href": "Comp/r-sas_linear-models.html",
    "title": "R vs SAS Linear Models",
    "section": "",
    "text": "Matching Contrasts: R and SAS\nIt is recommended to use the emmeans package when attempting to match contrasts between R and SAS. In SAS, all contrasts must be manually defined, whereas in R, we have many ways to use pre-existing contrast definitions. The emmeans package makes simplifies this process, and provides syntax that is similar to the syntax of SAS.\nThis is how we would define a contrast in SAS.\n\n# In SAS\nproc glm data=work.mycsv;\n   class drug;\n   model post = drug pre / solution;\n   estimate 'C vs A'  drug -1  1 0;\n   estimate 'E vs CA' drug -1 -1 2;\nrun;\n\nAnd this is how we would define the same contrast in R, using the emmeans package.\n\nlm(formula = post ~ pre + drug, data = df_trial) %&gt;% \n  emmeans(\"drug\") %&gt;% \n  contrast(method = list(\n    \"C vs A\"  = c(-1,  1, 0),\n    \"E vs CA\" = c(-1, -1, 2)\n  ))\n\nNote, however, that there are some cases where the scale of the parameter estimates between SAS and R is off, though the test statistics and p-values are identical. In these cases, we can adjust the SAS code to include a divisor. As far as we can tell, this difference only occurs when using the predefined Base R contrast methods like contr.helmert.\n\nproc glm data=work.mycsv;\n   class drug;\n   model post = drug pre / solution;\n   estimate 'C vs A'  drug -1  1 0 / divisor = 2;\n   estimate 'E vs CA' drug -1 -1 2 / divisor = 6;\nrun;"
  },
  {
    "objectID": "Comp/r-sas_manova.html",
    "href": "Comp/r-sas_manova.html",
    "title": "Multivariate Analysis of Variance in R vs SAS",
    "section": "",
    "text": "MANOVA: Testing for group mean vectors are the same vs at least one is different\nWhen applying the following hypothesis, SAS and R match identically see R and SAS.\n\nH0: Group mean vectors are the same for all groups or they don’t differ significantly.\nH1: At least one of the group mean vectors is different from the rest.\n\nHowever, if interest was in comparing 1 level of a parameter vs the others, this was only achieved using SAS. Contrast statements in SAS were easy to implement as shown here SAS however R did not replicate these results and to date a solution has not been found.\nNOTE: if you feel you can help with the above discrepancy please contribute to the CAMIS repo by following the instructions on the contributions page."
  },
  {
    "objectID": "Comp/r-sas_rounding.html",
    "href": "Comp/r-sas_rounding.html",
    "title": "R v SAS rounding",
    "section": "",
    "text": "Rounding; R and SAS\nOn comparing the documentation of rounding rules for both languages, it will be noted that the default rounding rule (implemented in the respective language’s round() function) are different. Numerical differences arise in the knife-edge case where the number being rounded is equidistant between the two possible results. The round() function in SAS will round the number ‘away from zero’, meaning that 12.5 rounds to the integer 13. The round() function in Base R will round the number ‘to even’, meaning that 12.5 rounds to the integer 12. SAS does provide the rounde() function which rounds to even and the janitor package in R contains a function round_half_up() that rounds away from zero. In this use case, SAS produces a correct result from its round() function, based on its documentation, as does R. Both are right based on what they say they do, but they produce different results (Rimler, M.S. et al.).\nReferences\nRimler M.S., Rickert J., Jen M-H., Stackhouse M. Understanding differences in statistical methodology implementations across programming languages (2022, Fall). ASA Biopharmaceutical Report Issue 3, Volume 29.  Retrieved from https://higherlogicdownload.s3.amazonaws.com/AMSTAT/fa4dd52c-8429-41d0-abdf-0011047bfa19/UploadedImages/BIOP%20Report/BioPharm_fall2022FINAL.pdf"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CAMIS",
    "section": "",
    "text": "Introduction\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Subtle differences exist between the fundamental approaches implemented by each language, yielding differences in results which are each correct in their own right. The fact that these differences exist causes unease on the behalf of sponsor companies when submitting to a regulatory agency, as it is uncertain if the agency will view these differences as problematic. In its Statistical Software Clarifying Statement, the US Food and Drug Administration (FDA) states that it “FDA does not require use of any specific software for statistical analyses” and that “the computer software used for data management and statistical analysis should be reliable.” Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\n\nMotivation\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\n\n\n\n\n\n\n\n\n\nMethods\nR\nSAS\nComparison\n\n\n\n\nSummary Statistics\nRounding\nR\nSAS\nR vs SAS\n\n\nSummary statistics\nR\nSAS\nR vs SAS\n\n\nGeneral Linear Models\nStudents t-test\n\n\n\n\n\nPaired t-test\n\n\n\n\n\nANOVA\nR\nSAS\nR vs SAS\n\n\nANCOVA\n\n\n\n\n\nMANOVA\nR\nSAS\nR vs SAS\n\n\nLinear Models - Simple regression\n\n\n\n\n\nLinear Models - Multiple regression\n\n\n\n\n\nGeneralized Linear Models\nLogistic Regression\n\n\n\n\n\nPoisson Regression\n\n\n\n\n\nNegative Binomial Regression\n\n\n\n\n\nCategorical Repeated Measures\n\n\n\n\n\nCategorical Multiple Imputation\n\n\n\n\n\nNon-parametric Analysis\nWilcoxon signed rank\n\n\n\n\n\nMann-Whitney U/Wilcoxon rank sum\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\n\nKruskall-Wallis test\n\n\n\n\n\nFriedman test\n\n\n\n\n\nJonckheere test\n\n\n\n\n\nCategorical Data Analysis\nBinomial test\n\n\n\n\n\nMcNemar's test\nR\nSAS\nR vs SAS\n\n\nChi-Square Association\n\n\n\n\n\nFishers exact\n\n\n\n\n\nCochran Mantel Haenszel\n\n\n\n\n\nConfidence Intervals for proportions\n\n\n\n\n\nLinear Mixed Models\nMMRM\nR\nSAS\nR vs SAS\n\n\nGeneralized Linear Mixed Models\nMMRM\n\n\n\n\n\nMultiple Imputation - Continuous Data MAR\nMCMC\n\n\n\n\n\nLinear regression\n\n\n\n\n\nPredictive Mean Matching\n\n\n\n\n\nPropensity Scores\n\n\n\n\n\nMultiple Imputation - Continuous Data MNAR\nDelta Adjustment/Tipping Point\n\n\n\n\n\nReference-Based Imputation/Sequential Methods\n\n\n\n\n\nReference-Based Imputation/Joint Modelling\n\n\n\n\n\nCorrelation\nPearson's correlation coefficient\n\n\n\n\n\nSpearman's correlation coefficient\n\n\n\n\n\nKendall's Rank Correlation\n\n\n\n\n\nChi-Square Correlation\n\n\n\n\n\nSurvival Models\nKaplan Meier\n\n\n\n\n\nLog-rank test\n\n\n\n\n\nCox-proportional hazards\n\n\n\n\n\nAccelerated Failure Time\n\n\n\n\n\nNon-proportional hazards methods\n\n\n\n\n\nSample size /Power calculations\nSingle timepoint analysis\n\n\n\n\n\nGroup-sequential designs\n\n\n\n\n\nMultivariate methods\nClustering\n\n\n\n\n\nFactor analysis\n\n\n\n\n\nPCA\n\n\n\n\n\nCanonical correlation\n\n\n\n\n\nPLS\n\n\n\n\n\nOther Methods\nNearest neighbour\n\n\n\n\n\nCausal inference\n\n\n\n\n\nMachine learning"
  },
  {
    "objectID": "minutes/index.html",
    "href": "minutes/index.html",
    "title": "Meeting Minutes",
    "section": "",
    "text": "White Paper, Website, Launch Plan\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhite Paper, Website, ONCO, Volunteers, Conferences\n\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhite Paper and Demo of connecting Rstudio with Github repo\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nNew Website Discussion\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRestart Meeting\n\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "minutes/posts/13Feb2023.html",
    "href": "minutes/posts/13Feb2023.html",
    "title": "White Paper and Demo of connecting Rstudio with Github repo",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\nattendees\n13_feb_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nYes\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nYes\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nNo\n\n\nVikash Jain\nNo\n\n\nWilmar Igl\nNo\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting minutes\nMin-Hua went through outstanding comments on the white paper.\nChristina did a demo of how to set up R studio to link through via git project to the CAMIS github repo. See “13Feb2023_Contributing to the CAMIS project_Setting up communication between github and R studio” for more information\n\n\nNext meeting: 13th March 2023: 4:30 UTC, 11:30 EST."
  },
  {
    "objectID": "minutes/posts/12Dec2022.html",
    "href": "minutes/posts/12Dec2022.html",
    "title": "Restart Meeting",
    "section": "",
    "text": "Attendees\n\n\nNew names:\nRows: 25 Columns: 7\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): attendees, 12_dec_2023, 23_Jan_2023, 13_feb_2023, 13_mar_2023, 17_a... lgl\n(1): ...7\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...7`\n\n\n\n\n\n\n\n\n\n\nattendees\n12_dec_2023\n\n\n\n\nAiming Yang\nNo\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nNo\n\n\nChristina Fillmore\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nHarshal Khanolkar\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nYes\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nPaula Rowley\nYes\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nNo\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome and brief CAMIS project update: Lyn\nPlease consider which areas of the project you would like to be involved with: \n    * Repository reviewers/framework reviewers\n    * Content creators (Comparing analysis method implementations in software)\n    * Github - content review / approval\n    * Marketing, i.e. blogs and sharing with wider community (PSI, ASA, PHUSE etc) to encourage contributions \n    * Long term plan - Extend reach beyond Europe/USA.\n\n\nRepository roadmap : Lyn\n    * Sample website & templates – mid January 2022\n    * Feedback on website/templates – EOB Feb 2022\n    * Revisions – March 2022\n    * Launch – April 2022\n\n\nWhite paper status update: Min-Hua\nNOTE: we would like to put the URL of new website and mention CAMIS in paper if possible?\n\n\nOther stream updates: All\nNeed to identify who were the previous stream leads to check with them we can put content into new template formats.\n    * CMH\n    * Mixed models\n    * Linear models\n\n\nQuestions/ AOB - All\n    * Future meeting plan – Lyn set up directly so can be quickly adjust/ add more meetings if necessary?\n    * Name change: CAMIS: Comparing analysis method implementations in software\n    * Do we need our own logo. CAMIS. Volunteers?\n    * Supported by PHUSE & PSI & ASA. Assign rep (or reps) for each organization. \n    * Extend membership given many previous members no longer on project\n    * Volunteer needed – can someone create a comparison using any method (but comparing SAS to Python/Julia or R to Python/Julia) – so we can test up with not just R Vs SAS.\n    * AOB."
  },
  {
    "objectID": "minutes/posts/23Jan2023.html",
    "href": "minutes/posts/23Jan2023.html",
    "title": "New Website Discussion",
    "section": "",
    "text": "attendees\n23_Jan_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nHarshal Khanolkar\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nYes\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nYes\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVikash Jain\nNo\n\n\nWilmar Igl\nNo"
  },
  {
    "objectID": "minutes/posts/23Jan2023.html#christina-provided-a-summary-of-work-to-date-on-the-website",
    "href": "minutes/posts/23Jan2023.html#christina-provided-a-summary-of-work-to-date-on-the-website",
    "title": "New Website Discussion",
    "section": "Christina provided a summary of work to date on the website",
    "text": "Christina provided a summary of work to date on the website\nRepo now live: [https://psiaims.github.io/CAMIS/]\nPrimary mode of navigation will be the table of contents..\nComprehensive Search function is available to supplement the use of the TOC.\nThe website is build from 3 folders in github:\nR SAS Comp\nThese folders, map to the columns of the table, I.e. everything about R is in Quarto files under R.\nComp folder: for the Comparison – name sure you name the two software you are using r-sas - so we can use this when dynamically selecting.\nIn future we can add Python / Julia directories.\nThe idea would be for people to use the: [CAMIS/templates/R_template.qmd] - A template of how to write documentation for the R part of the site. They’d Edit template & save it back into the R folder naming it clearly for what it is. Template should also contain name packages being used at start of each method comparison. It’d be difficult to be exhaustive with all the survival analysis packages i.e. accelerated failure time packages, etc.., but as long as stated hopefully can grow over time.\nThe Data-info folder – contains description of all data being used for the comparisons. Going forward if different data used, the information about the data would be put into this folder. This allows the data description to sit outside of the comparison folders & where possible same data be used across comparisons.\n\nQuestions & Discussion\nJoe & Michael raised that the About tab which has information about the project is out of date, so should be updated. We also have no detail on the driving mechanism… I.e. what we would like from collaborators. Add “How to collaborate” button.\nItems to be discussed further which may need to be included in the site:\n\nupdate Methods: needs to make it more robust to future uploads - i.e. topics within linear models? (Sub categories) focus on methods, but how sort the methods for inclusion of all in future\nRating the software discrepancies. I..e How severe the difference is?\nNeed to create a template for comparisons. Discuss if we would have a purpose/highlight of comparison/ summary/conclusions at the top first. Also if we put List of R packages this comparison uses (use Tags?) - Need to consider if package superseeded/ multiple packages whether they go in 1 document or multiple.\nHow to expand to sort by: therapeutic area relevance (would be good to link from methods to Oncology somehow\nWhat if a different package.., does same analysis… have to make it clear which package is being used & include multiple packages. It was agreed that as long as we are clear on what we have compared then Its ok to not be all inclusive. That can be added by other collaborators later. It was noted by Kyle that for survival (I.e. accelerated failure time packages), it may be hard to include all. The recommendation is to start with 1 and can expand further as it grows. We may have to re-think website design as it grows to accommodate. Hence why we want everything written in smaller parts to can easily manipulate going forward."
  },
  {
    "objectID": "minutes/posts/23Jan2023.html#min-hua-provided-an-update-on-the-white-paper",
    "href": "minutes/posts/23Jan2023.html#min-hua-provided-an-update-on-the-white-paper",
    "title": "New Website Discussion",
    "section": "Min-Hua provided an update on the white paper:",
    "text": "Min-Hua provided an update on the white paper:\nIn its final stages of review by team, and will now be sent for wider review."
  },
  {
    "objectID": "minutes/posts/17apr2023.html",
    "href": "minutes/posts/17apr2023.html",
    "title": "White Paper, Website, Launch Plan",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\nattendees\n17_apr_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nClara Beck\nNo\n\n\nAditee Dani\nYes\n\n\nDoug Kelkoff\nYes\n\n\nHarshal Khanolkar\nYes\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nYes\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nNo\n\n\nVikash Jain\nNo\n\n\nWilmar Igl\nNo\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\n\nWhite paper - Lyn/Min-Hua\nLogo - All - voting!\nWebsite progress - Christina\nLaunch Plan - Lyn\nCAMIS-ONCO - Soma Sekhar\nVolunteer Open Roles\nConference Attendance\nAOB\n\n\n\nMeeting minutes\nWhite Paper Update: Min-Hua PHUSE are doing technical review so hopefully will come back shortly with any comments. I reminded them last week. Has been reviewed by leads team, now with steering committee ( Final review team), so hopefully not much longer.\nLOGO: Lyn - By a small majority the preferred option was the calculator without the P&lt;0.05 in it. This will now be redrawn & finalized. ACTION: Lyn will update website when image available. Will save under CAMIS/images so you can use for any posters/ presentations.\nWebsite progress: Christina **All - review of progress & answer any questions\nSurvival - Mia has made great progress on survival, Christina and Lyn to help fix branch issue & then will get it pushed to the live site.\nACTION : Lyn to Create a video of creating a branch / doing updates. push/pull - github pull request. Create a FAQ doc for the website.\nLaunch Plan\n\nAlign launch of website with release of white paper. Blog writing & “Video” launch - Lyn to write & distribute for review\nOnce content created reach out to the following to help advertise\n\nPSI /EFSPI (Martin),\nR Consortium / PHUSE / RSS (Lyn)\nIASCT (Harshal)\nASA (Min-hua may have contact or See if Ben has a contact- ACTION christina to check with ben then get back to Min-hua. Lily Hsieh to ask Leon as he’s part of ASA. Aiming can also reach out to a contact to see she has a contact )\nOthers : TBC\n\n\nCAMIS- ONCO: Soma Sekhar\nPlans are in progress\nReview of volunteer open roles Still looking for volunteers to do: - Co-ordinator for conference material - share standard slides/ content /abstracts /posters - Social media rep - to co-ordinate posts (linkedIn/Twitter) - Volunteers to represent CAMIS at various conferences\nConferences All to let Lyn know or update the conferences qmd if you want to attend and represent/advertise camis"
  },
  {
    "objectID": "minutes/posts/13mar2023.html",
    "href": "minutes/posts/13mar2023.html",
    "title": "White Paper, Website, ONCO, Volunteers, Conferences",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\nattendees\n13_mar_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nClara Beck\nNo\n\n\nAditee Dani\nYes\n\n\nDoug Kelkoff\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nYes\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nNo\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nNo\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\n\nWhite paper - Lyn\nWebsite progress - Christina\nCAMIS-ONCO - Soma Sekhar\nVolunteers Roles\nConference Reps\nAOB/ PHUSE feedback\n\n\n\nMeeting minutes\nWhite Paper Update: Lyn Min-Hua is sending the white paper to the PHUSE group lead for review soon just a bit more tidying to do following comments.\nWebsite progress: Christina\nThe home page now has the list of stats methods we are looking to collect data on.\nACTION :Christina to put actions for each stats method which we need help to complete into github. We will assign those already selected to the people below. This will enable people that want to help to be able to see which are available for people to select.\nCAMIS- ONCO: Soma Sekhar\n\nplan to launch later this week.\npossibly white paper/ conference presentation.\n\nReview of volunteer roles\n\nGeneral Co-ordination -Lyn\nWebsite Co-ordination / Home page table - Christina\nCAMIS-ONCO - Soma Sekhar\nCopying CSRMLW material to CAMIS\n\nCMH: Aiming Yang\nLinear Models: Brian Varney (ACTION: Set up call with Dani, Lyn, Vikash, christina, + anyone else whose interested in helping to please volunteer)\nMMRM: Ben Arancibia\nSurvival: Min-Hua Jen, Mia Qi\n\n\nACTION: Christina to also add “actions” for people to pick up the following duties.\n\nCo-ordinator for conference material - share standard slides/ content /abstracts\nVolunteer to design a CAMIS Logo\nSocial media rep - to co-ordinate posts (linkedIn/Twitter)\nConference reps/ attendees needed\n\nWe will also add a page which lists the conferences so we can collate and coordinate whose going with the hope of advertising the project more widely once we have content on the website. Include a column for timelines/ abstract deadlines.\n\n\n\n\n\n\n\n\n\n\n\nConference\nLocation\nDate\nMain Contact\nVolunteers to attend\nDetails\n\n\n\n\nPHUSE US Connect\nOrlando, Florida\n5-8 March 2023\nSoma Sekhar\n\nPresentation\n\n\nDISS (Duke industry statistics symposium)\nVirtual\n29-31st March 2023\nLyn Taylor\nMolly MacDiarmid\nPoster\n\n\nPSDM(Pharmaceutical statistics and data management)\nNetherlands\n19 Apr 2023\n\n\n\n\n\nIASCT (ConSPIC - conference for statistics and programming in clinical research)\nBengaluru, India\n4-6 May 2023\nHarshal Khanolkar\nHarshal Khanolkar\n\n\n\nPSI 2023 Conference\nHammersmith London West, England\n11-14 June 2023\nMartin Brown\nChristina Fillmore\nLyn Taylor\nMolly Macdiarmid\nMartin Brown\nOral & poster submission completed\n\n\nDIA 2023 Global Annual Meeting\nBoston MA, USA\n25-29 June 2023\n\n\n\n\n\nJoint statistical meeting (JSM)\nToronto, Ontario, Canada\n5-10 Aug 2023\n\n\n\n\n\nISCB Conference\nMilan-Italy\n27-31 Aug 2023\n\n\n\n\n\nRSS conference\nHarrogate, England\n4-7 sept 2023\n\n\n\n\n\nASA Bio pharmaceutical Section Regulatory-industry Statistics Workshop\nRockville, Maryland, USA\n27-29 Sept 2023\n\n\n\n\n\nEASD 2023 - European Association for study of diabetes\nHamberg Germany\n02-06 Oct 2023\n\n\n\n\n\nPHUSE EU Connect 2023\nICC Birmingham, England\n5-8 November 2023\nChristina Fillmore?\n\n\n\n\nR in Pharma /\nPOSIT conf.\nVirtual/ In person\nNov?\nChristina Fillmore?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "CAMIS: Comparing Analysis Method Implementations in Software\nWe are a cross-industry group formed of members from PHUSE, PSI, and ASA."
  },
  {
    "objectID": "about.html#who-are-we",
    "href": "about.html#who-are-we",
    "title": "About",
    "section": "",
    "text": "CAMIS: Comparing Analysis Method Implementations in Software\nWe are a cross-industry group formed of members from PHUSE, PSI, and ASA."
  },
  {
    "objectID": "about.html#objectives",
    "href": "about.html#objectives",
    "title": "About",
    "section": "Objectives",
    "text": "Objectives\nThrough the creation of a white paper the group will provide guidance on the types of questions statistical staff should ask to identify the fundamental sources of discrepant results between software.\nThe group also aims to save unnecessary repetition of work within the community, through the creation of this open source repository. This repository welcomes contributions from the wider community and is a resource comparing and documenting differences in analysis method implementations in software."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nTraditionally, highly regulated industries (such as the pharmaceutical industry), have limited themselves to the use of commercially available software. When taking such an approach, the responsibility for the validation and testing of the product was often delegated to the software development company themselves, to ensure the software performs in line with its documentation, producing accurate reliable and reproducible results. However, one downside of this approach is that new methods and functionality can be slow to be adopted, limiting new method implementation and tools that can bring in efficiency.\nWith the increase in popularity of data science, the rate at which community led tools and methods are being developed in open-source software is rapid. The availability of advanced analytic capabilities, has led to increased desire for statistical staff in regulated industries to have access and approval to use to open source software (Rimler et al. 2022). The use of open source software is now widely accepted (FDA 2015), however, this increased variety of tools has resulted in an overlap of capabilities. This has raised challenging questions of traditional approaches to clinical analyses – particularly in situations where the overlap yields different results.\nOne example of this challenge encompasses discrepancies which have been discovered in statistical analysis results between different programming languages, even when working within qualified statistical computing environments. Subtle differences exist between the fundamental approaches and assumptions implemented within each language, yielding differences in results which are correct and consistent with their respective documentation. The fact that these differences exist may cause unease for sponsor companies when submitting to a regulatory agency, as it is uncertain if the agency will view these differences as problematic. By understanding the source of any discrepancies, one can reinstate that confidence.\nThis cross-industry group aims to empower statistical staff to make informed choices on the implementation of statistical analyses when multiple languages yield different results."
  },
  {
    "objectID": "about.html#references",
    "href": "about.html#references",
    "title": "About",
    "section": "References",
    "text": "References\n\nMichael S. Rimler, Joseph Rickert, Min-Hua Jen, Mike Stackhouse. 2022. Understanding differences in statistical methodology implementations across programming languages. BioPharm_fall2022FINAL.pdf (higherlogicdownload.s3.amazonaws.com)\nhttps://www.fda.gov/downloads/ForIndustry/DataStandards/StudyDataStandards/UCM587506.pdf\nNOTE: White paper will be cited when published"
  },
  {
    "objectID": "contribution.html",
    "href": "contribution.html",
    "title": "Contributions",
    "section": "",
    "text": "Request for Contributions\nAlthough this project does have a core team, the endeavor of tracking all these comparisons will fail without community contributions. We welcome a wide verity of contributions from correcting small typos all the way to full write-ups comparing software (languages) for a method.\nPlease contribute by submitting a pull request to and our team will review it. If you are adding a page please follow one of our templates:\n\nR template\n\nInstructions for Contributions to the CAMIS repository\n\nSet up RStudio to clone the CAMIS github repo – See this guidance for more detail\nIf this is your first contribution, contact christina.e.fillmore@gsk.com and give her your github username, requesting to access the CAMIS repo for contributions\nGo into RStudio and Create a branch –Within RStudio click the branch button (on the git tab top right). Within the box that comes up ensure you are on the “remote=origin” and “Sync branch with remote” is checked. You can name the branch something to do with the amends you intend to make.\nEdit and /or add files within the CAMIS directories. If you are adding SAS guidance store under sas folder, R guidance store under r folder, for “SAS vs R” comparison store under comp. Follow the naming convention of the files already stored in those folders.\nWithin R studio - Commit each change or new file added, and push to the repo from within R studio.\nGo into github and do a pull request to sync your branch back to the origin. See create a pull request for more detail. Note that your change will need a reviewer, so please add DrLynTaylor and statasaurus as reviewers.\nOnce your change is approved, and merged into the origin, the branch will be deleted and you will need to create a new branch to add further contributions. NOTE: you can make the new branch called the same as the old one if you wish but ensure you select to overwrite the previous one."
  },
  {
    "objectID": "Conferences.html",
    "href": "Conferences.html",
    "title": "Conferences",
    "section": "",
    "text": "Conference Programme\nWe plan to showcase the CAMIS project at a number of conferences throughout 2023 and 2024. See below the list of conferences the CAMIS team will be at and please come say hello to us !\n\n\n\n\n\n\n\n\n\n\n\nConference\n2023/2024 Planning dates\n2023 Date & Location\n2023 Main Contact (also attending)\n\nDetails\n\n\n\n\nPHUSE US Connect\nTBC for 2024\n5-8 March 2023 Orlando, Florida\nSoma Sekhar\n\nPresentation\n\n\nDISS (Duke industry statistics symposium)\nTBC for 2024\n29-31st March 2023 Virtual\nMolly MacDiarmid\n\nPoster\n\n\nPSDM(Pharmaceutical statistics and data management)\n\n19 Apr 2023 Netherlands\n\n\n\n\n\nIASCT (ConSPIC - conference for statistics and programming in clinical research)\nAbstract submission 14 Mar-3rd Apr\n4-6 May 2023 Bengaluru, India\nHarshal Khanolkar\nHarshal Khanolkar\nTalk. and/or poster\n\n\nPSI 2023 Conference\nTalk submission Nov.\nPoster submission Feb.\n11-14 June 2023 Hammersmith London West, England\nMartin Brown\nChristina Fillmore\nLyn Taylor\nMolly Macdiarmid\nMartin Brown\nOral & poster submission completed\n\n\nDIA 2023 Global Annual Meeting\n\n25-29 June 2023 Boston MA, USA\n\n\n\n\n\nJoint statistical meeting (JSM)\n\n5-10 Aug 2023 Toronto, Ontario, Canada\n\n\n\n\n\nISCB Conference\n\n27-31 Aug 2023 Milan-Italy\n\n\n\n\n\nRSS conference\nAbstract by 6th April\n4-7 sept 2023 Harrogate, England\nLyn Taylor\n\nPoster/talk TBC?\n\n\nPHUSE CSS\nTBC\nSept 18-20, Maryland USA\nVikash Jain (TBC)\n\n\n\n\nASA Bio pharmaceutical Section Regulatory-industry Statistics Workshop\n\n27-29 Sept 2023 Rockville, Maryland, USA\n\n\n\n\n\nEASD 2023 - European Association for study of diabetes\n\n02-06 Oct 2023 Hamberg Germany\n\n\n\n\n\nPHUSE EU Connect 2023\n\n5-8 November 2023 ICC Birmingham, England\nChristina Fillmore\n\n\n\n\nR in Pharma /\nPOSIT conf.\n\nNov? Virtual/ In person\nChristina Fillmore\nR Validation Hub team will include a slide for us. (Juliane Manitz/Doug Kelkhoff)\nAbstract submitted for talk through R valid hub."
  }
]